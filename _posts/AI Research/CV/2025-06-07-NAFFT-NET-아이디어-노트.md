---
title: NAFFT-Net 아이디어 노트
categories:
   - CV
tags:
   - CV
---

# NAFNet 구현
> 학습 최적화 기법 참고 [링크](https://medium.com/curg/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%84%B1%EB%8A%A5%EC%9D%84-%EB%86%92%EC%9D%B4%EA%B8%B0-%EC%9C%84%ED%95%9C-%EB%8B%A4%EC%96%91%ED%95%9C-%EA%BF%80%ED%8C%81%EB%93%A4-1910c6c7094a)
> 실제 NAFNet은 width=64, batch=64, iter=400K → epoch=3053

> **3053 epoch는 돌려야함**



## Dataset 시행착오

### 1. Transform Resize 진행

#### 1-1. 결과

> 21epoch 기준 Resize이미지에는 25dB, full-size 이미지에는 20dB

#### 1-2. 고찰

**Resize를 사용할 시, 256x256가 아닌 고해상도 사진 추론 시, 학습효과 없었음**

> Resize하며 찌그러진 사진을 가지고 학습하기 때문



### 2. RandomCrop 사용

#### 2-1. 결과

> Resize의 단점을 보완하고자 RandomCrop 사용하여
>
> > - PSNR 확인 불가 (추후 추가 예정)

#### 2-2. 고찰

- 결과에 대한 고찰

> epoch가 늘어남에도 PSNR이 개선되지 않음에, 아래의 이유로 학습이 어려워서 그런거라고 판단
>
> - Full-size의 원본 이미지에서 256x256 부분만 랜덤하게 Crop해 오기 때문에, 학습자료의 연속성이 없음
> - 매번 새로운 train set을 마주하니 적은 epoch으로는 학습이 어려움

- 구현에 대한 고찰

> Random crop을 적용할 때, Low quality Image와 Ground Truth Image가 '이미지의 같은 부분'이어야 하는데,
>
> 둘다 다르게 되는 문제가 발생하여 이를 방지하고자, Transform 함수를 재정의해서 2개의 image를 사용할 수 있게함

```python
# 이 과정에서 이런 함수로 생각하고 오류가 났었음
import torch.nn.functional as F

# 따라서 아래와 같이 수정
import torchvision.transforms.functional as TVF
```



### 3. Full-image를 소분함

#### 3-1. 결과

> image를 512x512로 서로 256은 겹치게 4개로 쪼갬; 데이터 개수 또한 기존 2103에서 8412로 4배 증가
>
> > 12epoch에 24.74dB

#### 3-2. 고찰

> 이제 12 epoch 보다 훨씬 더 많이 돌려보며 PSNR을 확인할 계획

- 소분을 실행할 코드

```utils/disassemble_ds.py```

----

**dataset과 전처리 문제는 종결하고, 새로운 Network를 연구하기 시작**

---

## Network 연구

### **IDEA**

- Sobel Loss?
  - Laplacian On Gaussian 사용하는게 좋을듯
- pixel만 point by point로 비교하며 작동하니깐 문제가발생
  - 통쨰로 움직일수있도록 설정하는게 좋을거같아
- **more frame을 만드는 친구들을 이용해서 어떻게 이동해가는지 확인해보는것도 좋을듯**
- NAFNet에서 Encoding block 개수를 줄이는 대신 DO Conv Block을 넣는게 나을지도

- JPEG의 압축방식을 이용해서 중간에 있는 데이터들은 어떻게 되는지 보고
  - 데이터셋을 늘려서 이동정보를 파악하는 방식은 어떨까?



- HSV 색공간을 사용하는건 어떨까? Edge detecting이라던가 이런 업무를 볼때 자주 사용하는 색 공간..

- FFT 사용 (Fast Fourier Transform)

  > 요즘 Deblur에 다양한 방식을 결합하는것을 시도하는중

  - Twin-transformer 구조를 이용하여, 흐려짐이 발생한 위치에 Focus를 해서 Deblur를 한다던가
  - FFT를 이용하여 Deblur를 하는 방식들을 이용

  **따라서 Twin-transformer를 써서 흐려짐이 발생한 위치를 FFT로 교체해보면..?**

  

- MobileNetV3

- SPP-Net

  - crop이나, resize하지않고 전체적인 이미지를 볼 수 있게하는 Network

    > 마지막에 FC Layer를 통과시키기 위해서 resize가 필요했는데, 이를 SPP를 이용하여 Flatten 함으로서 resize를 안해도되게 바뀐거

    - 하지만, Object detection이 아니기때문에, 마냥 전체적으로 보는게 의미가 있나 싶기도함...
    - 또한 고정된 크기의 Feature를 생성하는점이 조금 걸림.

  - **NAFNet에 Resize나 FC Layer를 사용하는 부분이 없어서 사용 불가능**

    


### 1. ECA-Net: Effecient Channel Attention (NAFNet + 1dConv)
> [링크를 통해 ECA-Net 학습](https://eehoeskrap.tistory.com/480)

#### 1-1. 아이디어

- CA자체는 검증되어온 테크닉 (NAF논문의 37, 8항목 참조)

- SEblock는 Channel Attention의 한 종류로써, Excitation을 통해 특정 채널이 아닌, 전체적인 채널의 Attention을 고루 볼수 있게함
    > [링크의 3-1 Gating-Mechanism 항목 참고](https://ech97.tistory.com/entry/seblock)
    - 하지만 Excitation에서 두개의 FC Layer를 사용하며 문제가 발생함
        - 연산량이 너무 많음
        - 많은 연산량을 줄이기위해 Dimension을 Reduction하게되는데 이때, 정보 손실이 발생함

- ECA-Net은 FC layer 대신 1D Conv를 이용하여 연산량 문제를 해결하면서도, Attention normalize를 함
    - 하지만 FC Layer를 대체하기에는 1D Conv의 Kernel size(시야)가 크지 않으므로, 이를 잘 설정해줘야함
    - 채널의 수와 비례하도록 설정
        - ks_temp = int(abs((log(C, 2) + b) / gamma))   # gamma = 2, b = 1
        - kernel_size = ks_temp if ks_temp % 2 else ks_temp + 1     # 홀수 여야 하므로
    - **다만 연산량은 (얼만큼?)개선하였으나, Global한 성질을 살짝 잃긴했다**

#### 1-2. 결과

> **SCA의 point-wise conv (2d-Conv; MLP)를 ECA 논문에 맞게 1d conv로 변경한 결과**
>
> > - PSNR 확인불가 (추후 추가 예정)
> >

#### 1-3. 결과에 대한 고찰

> 아무리 실험환경이 3000 epoch 중 78 epoch라 하더라도, PSNR 증가 속도가 너무 느림.

- 다른 Loss를 써볼까 고민하게 됨.

- Block의 개수를 줄여볼까도 고민하게 됨

- low quality / predicted image / ground truth

  > 번지게 된 이동정보만 알아도 복구할수있는 이미지에서도 낮은 성능을 보임
  >
  > - 물론 학습이 덜된게 원인일 가능성도 있지만, **이동정보를 포함하거나, 번짐에 대한 추가적인 정보를 던져주면 잘 파악하겠다는 Idea를 떠올림**

![image-20221127145243004](https://raw.githubusercontent.com/ech97/save-image-repo/master/image/image-20221127145243004.png)

### 2. NAFNet + 1dConv + FFT + FFTLoss (확실히 윤곽에 강함을 확인할 수 있었음.)

#### 2-1. 아이디어

- 단순 Convolution Network는 윤곽선 및 엣지 추출에 특화되어있음

- 하지만 흐림영상의 대부분은 Low-frequency 성분이 많음

  <img src="https://raw.githubusercontent.com/ech97/save-image-repo/master/image/image-20221201121710414.png" alt="image-20221201121710414" style="zoom:67%;" />

- 따라서 Low-frequency의 정보또한 수집할 수 있어야하기때문에 (**low freq 제거를 위해. 그리고 결과적으로 윤곽을 빠르게 찾아나감**)

- FFT Block과 FFT Loss 개념을 추가하였음

  - FFT Block: fft → 1d conv - relu - 1d conv → ifft

    - Encoder와 Decoder 사이를 연결하는 Skip connection 부분에 적용
    - 처음에는 Encoding / Decoding block에 FFT Block을 잔차로 삽입할까 하였으나, 모델이 무거워질거같아 skip connection에만 적용

  - Loss: FFT Loss + PSNR Loss 사용

    > Loss에 따라 모델이 뭐에 민감한 모델이 되는지 결정되는데...
    >
    > FFT에도 민감하고, Edge에도 민감하게 하고싶었으나, 시간의 제약상 PSNR Loss만 사용하기로함

#### 2-2. 결과

> - PSNR 확인불가 (추후 추가 예정)

#### 2-3. 고찰

- Low Freq를 빠르게 제거하기때문에, 윤곽을 빠르게 찾아나가는것을 확인할 수 있었음
- 하지만 개선속도가 FFT가 없었을때보다 훨씬 느림을 알 수 있었음. 
- FFT Loss가 추가됨에 따라 학습이 어려워진것으로 예상됨
  - L1이 지적한 문제점을 FFT가 다른곳이 더 문제라고 소리쳐대니깐...
    - 이를 해결하고자 FFT Loss의 비율을 낮춤 0.05 → 0.001
      - 이렇게 학습하니 L1 Loss대비 작은 Loss때문에 이미지의 방향정보가 뒤틀리는 결과를 확인할 수 있었음
      - 따라서 FFT Loss가 작을바엔 지우는게 낮겠다 싶은 결론에 도달함

### 3. NAFNet + 1dConv + FFT + FFTLoss + DO-Conv - Encoding Layer (연산량 줄여보려함)

#### 3-1. 아이디어

- FFT 학습속도를 개선해줄만한 FFT 관련 모델이 있는지 찾아보다 발견함
- DeepRFT 모델은 DOConv를 이용함
- 무거운 기존 NAF 모델을 간략화하는대신 DO-Conv를 이용해볼까하는 아이디어를 떠올림
  - 인코딩의 마지막 단계. 즉 제일 의미를 함축하고있는 단계에서 기존에는 28개의 block을 사용하는것을 12개로 압축시킴.
  - 대신 Over parameterized Convolution을 사용
  - !!! Receptive field가 큰게 마냥 중요한것은 아님. 손만 움직이는건 어떻게 해결.
- FFT Loss는 0.01로 설정

#### 3-2. 결과

> PSNR 확인불가 (추후 추가 예정)

- 역시 FFT는 FFT인게 윤곽정보를 빠르게 찾아나감을 확인할 수 있었음
- 하지만 학습속도는 여전히 별로,,,
  - FFT Loss가 대략 L1Loss에 6~10배... 그냥 버리기로

![image-20221129210044513](https://raw.githubusercontent.com/ech97/save-image-repo/master/image/image-20221129210044513.png)

- 129epoch_514

![image-20221130115719856](https://raw.githubusercontent.com/ech97/save-image-repo/master/image/image-20221130115719856.png)

- 287 vs 140

  ![image-20221201121536765](https://raw.githubusercontent.com/ech97/save-image-repo/master/image/image-20221201121536765.png)

- epoch:300

  ![image-20221201154105354](https://raw.githubusercontent.com/ech97/save-image-repo/master/image/image-20221201154105354.png)

- 628_1028..?

  ![image-20221203145001438](https://raw.githubusercontent.com/ech97/save-image-repo/master/image/image-20221203145001438.png)

#### 3-3. 고찰

- 확실히 모델이 가벼워서 빠른 학습 가능
- epoch: 282~285 / blur가 강한경우는 회복이 어려운 경우가 많음.
  - **위치정보가 있으면 이런거에 더 강할텐데....**
  - 얘네는 1대1 대조식이기때문에 한계가 있음
- epoch: PSNR

---

### 4. NAFNet + 1dConv + FFT + DO-Conv + Encoding Layer - FFTLoss (연산량 증가. 추론에선 아님)

#### 4-1. 아이디어

- 결국, Encoding Layer가 충분하지 못하면, 강하게 Blur가 된거를 회복하지 못하기때문에 Encoding 늘려줌.

  - 또한 Layer를 적층시키고, 많은 Parameter를 뽑는것이 Blur가 심한이미지에 도움이 되는것을 확인했으므로,

  - Layer를 늘려야하나... 연산량 증가와 과적합이 우려되어 DO-Conv로 대체하는것으로 마무리 (DeepRFT 논문에서의 DO-Conv의 효과를 뭘로봤는지 써놓으면 더 좋을듯! // Deblurring 성능을 향상시키기위해 DO-Conv를 이용했다고만 되어있네... **DO-Conv의 논문을 참고해서 작성해야겠다.**)

    - (이를 뒷받침 하기 위해서는 DO-Conv가 있고 없고의 결과를 보여주는것이 필요할듯 PSNR이나 결과 Image나

      일정 epoch를 기준으로 발표자료 준비가 필요하겠다..)

- 이떄 FFT Loss를 이용하면 오히려 학습을 어려워하고, 일정 epoch이후로는 횡보하는 모습을 보여줌 (24dB 고정)

  - 또한 윤곽을 찾는데 있어서는 학습과정중 FFT Block을 잔차로 넣어주는것만으로도 의미있는 결과를 얻고있다고 판단.
  - (Blurry.png의 FFT 변환 결과를 통해서 확인시켜주면될듯. )

  - 따라서 **L1Loss만 사용하기로 확정**



#### 4-2. 결과

> - PSNR 확인불가 (추후 추가 예정)



> 나중에 그냥 비슷한 종류의 사진으로 한번 평가돌려서 레포트 쓰기

- epoch 105

  ![image-20221204142240007](https://raw.githubusercontent.com/ech97/save-image-repo/master/image/image-20221204142240007.png)

  - 지금꺼 133 epoch vs L1Loss-1dconv만 적용시킨거 153 epoch

    ![image-20221204192954187](https://raw.githubusercontent.com/ech97/save-image-repo/master/image/image-20221204192954187.png)

    ![image-20221204192917018](https://raw.githubusercontent.com/ech97/save-image-repo/master/image/image-20221204192917018.png)

  - 지금꺼 130 epoch vs 지금꺼 121 epoch vs 바로 직전 FFTLoss 347,,,, 잘떴네?

    ![image-20221204193022414](https://raw.githubusercontent.com/ech97/save-image-repo/master/image/image-20221204193022414.png)

    ![image-20221204193147941](https://raw.githubusercontent.com/ech97/save-image-repo/master/image/image-20221204193147941.png)

    ![image-20221204194616100](https://raw.githubusercontent.com/ech97/save-image-repo/master/image/image-20221204194616100.png)

